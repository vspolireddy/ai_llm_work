{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9253ccbb-f501-464f-9815-2d71783ff5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d8ff2c-29a2-4240-b476-74dac9a30612",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "754ab841-6dd9-4e3f-b769-864edbeaf1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5578c515-a619-4f71-9319-0bd292b51317",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fedc00de-139d-4551-9cea-ce5335aca693",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Models like GPT4o have been trained to receive instructions in a particular way.\n",
    "\n",
    "# They expect to receive:\n",
    "\n",
    "# A system prompt that tells them what task they are performing and what tone they should use\n",
    "\n",
    "# A user prompt -- the conversation starter that they should reply to\n",
    "\n",
    "def summarize_cv(cv_text):\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": f\"summarize the following CV:\\n\\n{cv_text}\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36f4ac6b-4895-46f3-915f-22d474c4f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cover_letter(cv_summary, job_description):\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a indespensible crafter of reading and selecting the skills sets and generating cover letters from a CV and also showing the name\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Using the following CV summary:\\n\\n{cv_summary}\\n\\nAnd the job description:\\n\\n{job_description}\\n\\nPlease write a personalized cover letter.\"}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c001539-4145-4d2f-b98e-209bdcb49001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Summary:\n",
      "**Summary of CV: Senior Data Engineer**\n",
      "\n",
      "The candidate is a Senior Data Engineer with a strong focus on innovation in data engineering, skilled in architecting, deploying, and managing complex data pipelines across various platforms. They possess expertise in utilizing cutting-edge technologies and machine learning frameworks to enhance data transformation, ingestion, and analytics, with a proven ability to optimize cloud resources for cost-efficiency and high performance.\n",
      "\n",
      "**Areas of Expertise:**\n",
      "- Spark/PySpark/Scala\n",
      "- Java/Python Data Analytics\n",
      "- AWS (Amazon Web Services)\n",
      "- Kubernetes/Knative\n",
      "- AWS SageMaker/Machine Learning\n",
      "- Data Analytics\n",
      "\n",
      "**Career Experience:**\n",
      "\n",
      "1. **Blue Cross Blue Shield NC (Remote) | Sr Data Analytics & Platform Engineer (08/2022 – 10/2024)**\n",
      "   - Developed event-driven Kafka pipelines and built enterprise microservices on Kubernetes.\n",
      "   - Managed data migration to Snowflake and optimized cloud usage with AWS services.\n",
      "   - Created robust monitoring systems and Docker images for seamless operational support.\n",
      "\n",
      "2. **NIKE (Contract) | Sr Data Engineer (08/2021 – 09/2022)**\n",
      "   - Built Python ETL pipelines in Apache Airflow and container-based solutions for data processing.\n",
      "   - Developed Spark JVM metrics collection tools and managed multiple complex DAGs in Airflow.\n",
      "\n",
      "3. **Blue Cross Blue Shield NC (Durham, NC) | Sr Data Analytics & Platform Engineer (07/2020 – 08/2021)**\n",
      "   - Transformed legacy ETL processes into contemporary Spark frameworks.\n",
      "   - Engineered APIs in PySpark, enhanced data ingestion processes, and automated job scheduling.\n",
      "\n",
      "4. **Lexis Nexis (Raleigh, NC) | Software Engineer III (07/2017 – 06/2020)**\n",
      "   - Architected data pipelines using Scala and managed cloud resources on AWS Kinesis and EMR.\n",
      "   - Deployed and orchestrated container services on Kubernetes, implementing serverless data pipelines.\n",
      "\n",
      "5. **United Health Group (Optum) (Raleigh, NC) | Big Data Developer (03/2016 – 11/2017)**\n",
      "   - Developed data processing scripts and optimized operations within a Hadoop environment.\n",
      "\n",
      "6. **Fidelity Investments (Raleigh-Durham, NC) | Java Developer & Intern (07/2014 – 08/2015)**\n",
      "   - Focused on database development using Oracle and Java, developing RESTful web services and data warehousing solutions.\n",
      "\n",
      "**Education:**\n",
      "- Bachelor of Science in Computer Science, East Carolina University\n",
      "- Coursework for Master of Science in Data Science, University of Wisconsin\n",
      "\n",
      "**Additional Experience:**\n",
      "- Application Support Engineer at Bank of America.\n",
      "\n",
      "This summary showcases the candidate’s extensive experience and technical skills in data engineering, particularly in cloud services, data processing, and analytics, along with a solid academic background.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the job description for the position you are applying for:\n",
      " spark\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Cover Letter:\n",
      "[Your Name]  \n",
      "[Your Address]  \n",
      "[City, State, Zip]  \n",
      "[Your Email]  \n",
      "[Your Phone Number]  \n",
      "[Date]  \n",
      "\n",
      "[Hiring Manager's Name]  \n",
      "[Company Name]  \n",
      "[Company Address]  \n",
      "[City, State, Zip]  \n",
      "\n",
      "Dear [Hiring Manager's Name],\n",
      "\n",
      "I am writing to express my interest in the Senior Data Engineer position at [Company Name] as advertised. With a strong background in data engineering, combined with a proven track record of developing and managing innovative, efficient data pipelines, I believe I possess the skills and experience necessary to make a significant contribution to your team.\n",
      "\n",
      "Throughout my career, I have consistently focused on maximizing the efficiency and performance of data processing systems. In my current role at Blue Cross Blue Shield NC, I have developed event-driven Kafka pipelines and built enterprise microservices on Kubernetes, which streamlined data workflows and improved operational efficiency. My ability to implement complex data solutions is backed by my extensive experience with Spark, PySpark, Scala, and cloud technologies such as AWS, enabling me to leverage the latest tools for data ingestion, transformation, and analytics.\n",
      "\n",
      "At NIKE, I honed my skills in Python ETL pipelines utilizing Apache Airflow, where I also developed Spark JVM metrics collection tools to ensure optimal performance. My passion for innovation is evident in my drive to transform legacy ETL processes into modern Spark frameworks that not only enhance data handling but also reduce processing times significantly.\n",
      "\n",
      "I am particularly excited about the opportunity at [Company Name] because of your commitment to leveraging advanced technologies to drive data insights. I am confident that my technical expertise, coupled with a strong foundation in data analytics and machine learning, aligns perfectly with your organizational needs.\n",
      "\n",
      "I hold a Bachelor of Science in Computer Science from East Carolina University, with further coursework in Data Science at the University of Wisconsin. This academic foundation, paired with my hands-on experience, allows me to approach data challenges with both theoretical and practical perspectives.\n",
      "\n",
      "I am eager to bring my skills in data architecture, cloud optimization, and analytics to [Company Name]. I welcome the opportunity to discuss how my experience aligns with your needs and how I can contribute to the continued success of your data initiatives. Thank you for considering my application. \n",
      "\n",
      "Sincerely,  \n",
      "[Your Name]  \n",
      "\n",
      "[Attachment: Resume]  \n",
      "\n",
      "(Note: Please fill in the placeholders like [Your Name], [Hiring Manager's Name], [Company Name], etc., with the relevant details.)\n"
     ]
    }
   ],
   "source": [
    "# Read CV from a text file\n",
    "try:\n",
    "    with open('data/cv.txt', 'r') as file:\n",
    "        cv_text = file.read()\n",
    "    \n",
    "    # Summarize the CV\n",
    "    cv_summary = summarize_cv(cv_text)\n",
    "    print(\"CV Summary:\")\n",
    "    print(cv_summary)\n",
    "\n",
    "    # Get job description from user\n",
    "    job_description = input(\"Enter the job description for the position you are applying for:\\n\")\n",
    "\n",
    "    # Generate cover letter\n",
    "    cover_letter = generate_cover_letter(cv_summary, job_description)\n",
    "    print(\"\\nGenerated Cover Letter:\")\n",
    "    print(cover_letter)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"The specified CV file was not found. Please ensure 'resume.txt' is in the correct directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805ac86-ac18-49ef-b30c-09262477751f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
