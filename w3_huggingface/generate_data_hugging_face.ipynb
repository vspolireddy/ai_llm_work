{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14470caf-18f1-451c-abd8-359cecaba4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ce4b0-70ef-4b3d-a762-85f3e496d58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)\n",
    "\n",
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeefd6be-bdf2-46b8-b476-208594dce546",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  device_map=\"auto\",\n",
    "  quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1301485-fba7-4150-a319-20e3a7f4b6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66ea60-4eca-42f6-81a5-b0f593ada611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(topic, number_of_data, inst1, resp1, inst2, resp2, inst3, resp3):\n",
    "    # Convert user inputs into multi-shot examples\n",
    "    multi_shot_examples = [\n",
    "        {\"instruction\": inst1, \"response\": resp1},\n",
    "        {\"instruction\": inst2, \"response\": resp2},\n",
    "        {\"instruction\": inst3, \"response\": resp3}\n",
    "    ]\n",
    "\n",
    "    # System prompt\n",
    "    system_prompt = f\"\"\"\n",
    "    You are a helpful assistant whose main purpose is to generate datasets.\n",
    "    Topic: {topic}\n",
    "    Return the dataset in JSON format. Use examples with simple, fun, and easy-to-understand instructions for beginners.\n",
    "    Include the following examples: {multi_shot_examples}\n",
    "    Return {number_of_data} examples each time.\n",
    "    Do not repeat the provided examples.\n",
    "    \"\"\"\n",
    "\n",
    "    # Example Messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Please generate my dataset for {topic}\"}\n",
    "    ]\n",
    "\n",
    "    # Tokenize Input\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n",
    "    streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    # Generate Output\n",
    "    outputs = model.generate(inputs, max_new_tokens=2000, streamer=streamer)\n",
    "\n",
    "    # Decode and Return\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def gradio_interface(topic, number_of_data, inst1, resp1, inst2, resp2, inst3, resp3):\n",
    "    return generate_dataset(topic, number_of_data, inst1, resp1, inst2, resp2, inst3, resp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d003d-1e8e-4652-a596-2e7abcadca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_topic = \"Talking to people about Gen Ai, what it means at the basic level and expand.\"\n",
    "default_number_of_data = 10\n",
    "default_multi_shot_examples = [\n",
    "    {\n",
    "        \"instruction\": \"What would be the benefit of AI?\",\n",
    "        \"response\": \"It is helping people retrive information faster\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"How should i use AI and ML?\",\n",
    "        \"response\": \"start by identifying specific tasks where they can provide value, such as content generation, data analysis, or automation\"\n",
    "    },\n",
    "    {\n",
    "        \"instruction\": \"why should I use AI and LLM?\",\n",
    "        \"response\": \"enhancing efficiency, creativity, and knowledge accessibility\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df5b4f-0dcd-4cc6-9ab2-522bc67f6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Topic\", value=default_topic, lines=2),\n",
    "        gr.Number(label=\"Number of Examples\", value=default_number_of_data, precision=0),\n",
    "        gr.Textbox(label=\"Instruction 1\", value=default_multi_shot_examples[0][\"instruction\"]),\n",
    "        gr.Textbox(label=\"Response 1\", value=default_multi_shot_examples[0][\"response\"]),\n",
    "        gr.Textbox(label=\"Instruction 2\", value=default_multi_shot_examples[1][\"instruction\"]),\n",
    "        gr.Textbox(label=\"Response 2\", value=default_multi_shot_examples[1][\"response\"]),\n",
    "        gr.Textbox(label=\"Instruction 3\", value=default_multi_shot_examples[2][\"instruction\"]),\n",
    "        gr.Textbox(label=\"Response 3\", value=default_multi_shot_examples[2][\"response\"]),\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Generated Dataset\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1f799-6490-4f27-a7b4-63297d997bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
